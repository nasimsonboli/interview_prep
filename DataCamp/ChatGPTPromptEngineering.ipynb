{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2d0027-138a-4a07-bf96-ff7693978591",
   "metadata": {},
   "source": [
    "# Prompt Engineering: How to write a quality prompt?\n",
    "- Clearly describe/ask the **task** and ask for a **clear direct objective**\n",
    "    - The clearer and more direct your prompt, the better ChatGPT understands your **intent**.\n",
    "    - Lack of clarity causes a misconception.\n",
    "- Give enough **context criteria** and **be specific**\n",
    "    - You can include an **example** in your prompt. \n",
    "- Make sure your prompt is **concise**\n",
    "    - Short and focused prompts often lead to better results s the model understands your intent faster and delivers more relevant results. **Longer prompts could lead to hallucinations** as they might overwhelm the model with too much information.\n",
    "    - Avoid unnecessary information.\n",
    "    - If a prompt is too vague and too **open-ended**, it can lead to fabricating information; however, the right level of it increases the **creativity** of the model and allows it to **think outside of the box**.\n",
    "\n",
    "- You can request the response from the point of view of a **persona**, **tone**, **format or style**\n",
    "- Ask ChatGPT to explain itself and provide **reasoning**.\n",
    "- **prompt evolution** and **refinement** is key. Prompts are static; they're dynamic. An iterative process, where the prompt is adjusted based on feedback, is key for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62458bb-8429-4907-a023-908d01da9ef5",
   "metadata": {},
   "source": [
    "# Common Pitfalls When Crafting Prompts\n",
    "- **Overloading** - too much (irrelevant) information\n",
    "- **Ambiguity** - vague prompts mean generalized answers\n",
    "- **Over-complication** - using jargon, complex phrasing, or technicalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b4587-efa9-4d94-a5b6-81e695991ed9",
   "metadata": {},
   "source": [
    "# Chain-of-thought (COT) prompting (Ask LLM to think step-by-step)\n",
    "\n",
    "Chain of Thought works by getting ChatGPT to break down a complex task into its subsequent steps. We are not only giving LLM examples but also roadmaps of how to arrive at an answer. By asking the model to reason through the conclusion, we are **giving it time to think**, which results in more accuracy.\n",
    "\n",
    "- **Zero-shot COT**: No structure or examples are provided. Instead, we describe a situation and its context. ChatGPT solves on its own. The model relies on its pre-training to generate a response that fits the prompts.\n",
    "    -  e.g., **\"Break down this problem step-by-step. Really think about it.\"**\n",
    "    -  By asking ChatGPT to reveal its reasoning, we gain insight into the model's thought process, allowing a better understanding of it and verifying its conclusion.\n",
    "- **One/Few-shot COT** - providing one or more examples to ChatGPT. Providing a roadmap of steps ChatGPT can follow to solve problems. We are training the model on the fly, giving it a richer context.\n",
    "    - It learns the steps to approach a particular problem(s) and reach a conclusion.\n",
    "    - One-shot might end up generating less precise answers compared to the few-shot technique.\n",
    "    - One fascinating aspect of few-shot learning is that ChatGPT becomes more than just an autocomplete tool. It turns into a **pattern-matching** and **pattern-recognition**.\n",
    "        - It analyzes the examples\n",
    "        - Mirrors the underlying patterns\n",
    "        - Creates new ideas\n",
    "    - e.g. Here's an example of a framework I use to problem solve: [insert steps]. Use this framework to solve the following problem: [insert problem]\n",
    "    - It can be personalized based on your examples. It can mimic your writing style or formatting preferences for reports to ensure consistency across documents, and it can use decision-making frameworks to generate new approaches to problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7d56a-8ef0-4c4e-8663-34514102b5c9",
   "metadata": {},
   "source": [
    "# LLM Training Techniques while prompting\n",
    "- zero-shot\n",
    "- one-shot\n",
    "- Few-shot learning\n",
    "\n",
    "The difference between these methods and COT is that we don't ask the model to reveal its reasoning here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5aa8b-8f17-476d-885d-1b8dafcadb6d",
   "metadata": {},
   "source": [
    "# Guide LLMs Response: SALT Framework\n",
    "Influence the model's response in a guided way by adjusting four specific properties of your prompt.\n",
    "\n",
    "- \"**S**tyle\" - define the framework for your desired response\n",
    "    - Ask for a structured output, like a list or a step-by-step guide.\n",
    "- \"**A**udience\" - tailoring the content\n",
    "    - This improves the relevance and makes it understandable for that specific audience \n",
    "- \"**L**ength\" - brevity or depth\n",
    "    - *Note that LLMs have a hard time sticking to hard constraints*. E.g., writing a paragraph that contains 100 words might not work. \n",
    "- \"**T**one\" - mood and feeling (e.g., formal, casual, playful, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e59a99-3e2a-44bc-9db2-f4d80c4869fc",
   "metadata": {},
   "source": [
    "# Prompt Formatting\n",
    "The way you present your prompt is as important as the content.\n",
    "\n",
    "- **Markdown** offers an elegant and straightforward way to structure text. e.g., \"Use the hash symbol **#** for headers, the number of # symbols indicates the level of the header.\"\n",
    "- **Bold**ing is used for highlighting by using double asterisks ** or double underscores -- before and after the text you want to bold.\n",
    "- **Quotation marks** highlight a word or phrase and allow ChatGPT to focus on the exact phrase, steering the response towards a moer specific and nuanced response. \n",
    "- **Delimiters** act as clear separators within your response. Triple dashes **(---)**  (e.g., between sentences) indicate distinct parts of the input so they are processed separately. It will separate the response based on these distinctions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11595ec-3587-4e20-9eb2-2f0ccc36fab1",
   "metadata": {},
   "source": [
    "# Chaining Prompts\n",
    "- Chaining prompts is building a narrative, where each question or task is a link in a chain that extends and **maintains the context**, **depth**, and **continuity** of the conversation.\n",
    "    - Adding \"continuing from our previous discussion,...\"\n",
    "    - Adding \"Based on the previous paragraph, ...\"\n",
    "- Encouraging the model to engage in deliberate thinking **prevents premature conclusions** and **ensures the model understands the task**.\n",
    "- Keeping the conversation going, allows us to **refine the prompt** for greater **relevance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d034465-6e37-4667-8ea7-ffbee2461de9",
   "metadata": {},
   "source": [
    "# Prompt Core Elements\n",
    "\n",
    "A great prompt has at least one of these elements\n",
    "- Instructions\n",
    "    - Breaking down the prompt into **step-by-step instructions** can dramatically enhance the depth of ChatGPT's response. By doing so, we're guiding the model through a structured thought process\n",
    "    - Most important elements are => Clarity, Specificity, Open-endedness\n",
    "- Context\n",
    "- Persona\n",
    "    Adopting a **persona** helps ensure the model's output is specific, relevant, and useful to the target audience.- \n",
    "- Output format\n",
    "    - SALT (Style, Audience, length, Tone), markdown, bolding, delimiters, quotation marks, etc.\n",
    "- Examples\n",
    "    - 0-shot, 1-shot, few-shot, COT\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84da5e8-2c3e-4520-be1c-e9295a285ce5",
   "metadata": {},
   "source": [
    "# Personal-problem-solution Framework\n",
    "- Ask it to answer the question from the **perspective of a certain persona**\n",
    "- Describe the **problem** \n",
    "- Ask for specific **solutions**\n",
    "    - asking to provide resources, references, a framework, or even a prompt to solve the problem\n",
    "    - \"step-by-step\" problem description and solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027dae7d-0702-468e-bed4-fa17f8b541f0",
   "metadata": {},
   "source": [
    "# A Prompt Template Example\n",
    "\n",
    "\n",
    "1) You are a **{Persona}**. You have been tasked with **{task}**.\n",
    "2) **{context}**\n",
    "3) Return **{output format}**\n",
    "\n",
    "4) A good example of this is given in backticks below.\n",
    "5) (Optional) `` `{example}` ``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bf0be-6251-430e-a436-b240f2d03ebb",
   "metadata": {},
   "source": [
    "# The four cornerstones of evaluation: LARF\n",
    "\n",
    "These four elements give you the skills to critically assess the outputs you receive from ChatGPT.\n",
    "\n",
    "- \"**L**ogical consistency\"\n",
    "- \"**A**ccuracy\"\n",
    "    - making sure LLM doesn't hallucinate. \n",
    "- \"**R**elevance\"\n",
    "    -  Response aligns with the **context** and **intent** of the prompt.\n",
    "- \"**F**actual correctness\" (beyond the cutoff date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b7eed-5054-4371-95ad-a84bb8827bf7",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "- Knowledge cut-off\n",
    "    - They are trained on data up to a certain date, any new information emerged after this cutoff date is not included.\n",
    "    - This influences the accuracy of response.\n",
    "    - Smart prompting can navigate this issue by adding **\"if you don't know this information and it is after your cutoff data, specify that you don't now\"**\n",
    "- Knowledge-base Imperfection\n",
    "    - **It's one-dimensional: You have the ask the questions from a certain direction to get answers.**\n",
    "    - They inherit and amplify biases in the data.\n",
    "- Discontinuous tasks vs. Incremental tasks\n",
    "    - LLMs are bad at discontinuous tasks such as\n",
    "        - Writing jokes\n",
    "        - Developing scientific hypotheses\n",
    "        - Creating new writing styles\n",
    "    - while they are good at incremental tasks that are solved sequentially, such as summarizing, answering questions, and imitating your writing style etc. It is because of overfitting and being only able to echo what they know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2024f8-2ed6-4ae7-a273-df72360f55fe",
   "metadata": {},
   "source": [
    "# Concerns\n",
    "- ChatGPT is a powerful tool, but it’s not always right since it generates responses **based on patterns, not verified facts**. It can produce **outdated or misleading information**, especially on recent events or complex topics.\n",
    "- Never input sensitive information like passwords, financial details, medical records, or anything you wouldn’t want shared publicly. **keeping prompts general and non-sensitive** ensures your privacy stays protected while still getting useful responses.\n",
    "- The model can present **biases** such as stereotypes or misinformation, **hallucinations** can also happen when the model confidently states incorrect information, or **overfitting** when the model is only as good as the data it's trained on.\n",
    "    - Recognizing and **cross-referencing** the answers is crucial to ensure **factual correctness**\n",
    "    - **Overfitting** occurs when the model is too close to its training data, making it hard to generalize to new, unseen data. So, the model will echo what it has seen in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b4ef4-05bb-4005-9ed7-6343ef7a9bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
